{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122701,
     "status": "ok",
     "timestamp": 1737283467810,
     "user": {
      "displayName": "Fawad Saqlain",
      "userId": "11871448354137572625"
     },
     "user_tz": -300
    },
    "id": "o43a2hrhE0qT",
    "outputId": "e1f80dba-3ab5-4c5d-d45a-66c2228120dd"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vKiyTsjGeyu"
   },
   "source": [
    "#  gpt complete code is following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRdq30oyZ2fk"
   },
   "source": [
    "# now making it beetter by adding apis datasets and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 25161,
     "status": "ok",
     "timestamp": 1737369450244,
     "user": {
      "displayName": "Fawad Saqlain",
      "userId": "11871448354137572625"
     },
     "user_tz": -300
    },
    "id": "bnux53dEhnwy"
   },
   "outputs": [],
   "source": [
    "import imaplib\n",
    "import smtplib\n",
    "import email\n",
    "from email.message import EmailMessage\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import sqlite3\n",
    "import uuid\n",
    "import os\n",
    "from datetime import datetime\n",
    "import chardet  # Library to detect character encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6d3lbpYMyAT"
   },
   "source": [
    "using instnwnd sql database   \n",
    "https://github.com/microsoft/sql-server-samples/blob/master/samples/databases/northwind-pubs/instnwnd.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 3052,
     "status": "error",
     "timestamp": 1737369463747,
     "user": {
      "displayName": "Fawad Saqlain",
      "userId": "11871448354137572625"
     },
     "user_tz": -300
    },
    "id": "1yO-WX3vRFOG",
    "outputId": "22eff30f-247a-466a-e6fa-e6f9830e84ff"
   },
   "outputs": [],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "\n",
    "# Configure the Google Generative AI API\n",
    "genai.configure(api_key=\"AIzaSyBA1FJ4OZsCDYla57Muc6EMS04ntEolrbE\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Load spaCy's model for Named Entity Recognition (NER)\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Database connection strings\n",
    "def create_connection():\n",
    "    try:\n",
    "        engine = create_engine(\"mssql+pyodbc://@127.0.0.1,1433/instnwnd?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        return None\n",
    "\n",
    "def conn_cursor():\n",
    "    conn_str = (\n",
    "        \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "        \"SERVER=127.0.0.1,1433;\"\n",
    "        \"DATABASE=instnwnd;\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "        \"Connection Timeout=60;\"\n",
    "    )\n",
    "    try:\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        print(\"Connected to MSSQL Server successfully!\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to MSSQL Server: {e}\")\n",
    "        return None\n",
    "\n",
    "# Map natural language columns to database schema\n",
    "TABLE_COLUMN_MAPPING = {\n",
    "    \"Employees\": [\n",
    "        \"EmployeeID\", \"LastName\", \"FirstName\", \"Title\", \"TitleOfCourtesy\", \"BirthDate\", \"HireDate\", \"Address\",\n",
    "        \"City\", \"Region\", \"PostalCode\", \"Country\", \"HomePhone\", \"Extension\", \"Photo\", \"Notes\", \"ReportsTo\", \"PhotoPath\"\n",
    "    ],\n",
    "    \"Categories\": [\"CategoryID\", \"CategoryName\", \"Description\", \"Picture\"],\n",
    "    \"Customers\": [\n",
    "        \"CustomerID\", \"CompanyName\", \"ContactName\", \"ContactTitle\", \"Address\", \"City\", \"Region\", \"PostalCode\",\n",
    "        \"Country\", \"Phone\", \"Fax\"\n",
    "    ],\n",
    "    \"Orders\": [\n",
    "        \"OrderID\", \"CustomerID\", \"EmployeeID\", \"OrderDate\", \"RequiredDate\", \"ShippedDate\", \"ShipVia\", \"Freight\",\n",
    "        \"ShipName\", \"ShipAddress\", \"ShipCity\", \"ShipRegion\", \"ShipPostalCode\", \"ShipCountry\"\n",
    "    ],\n",
    "    \"Products\": [\n",
    "        \"ProductID\", \"ProductName\", \"SupplierID\", \"CategoryID\", \"QuantityPerUnit\", \"UnitPrice\", \"UnitsInStock\",\n",
    "        \"UnitsOnOrder\", \"ReorderLevel\", \"Discontinued\"\n",
    "    ],\n",
    "    \"Shippers\": [\"ShipperID\", \"CompanyName\", \"Phone\"],\n",
    "    \"Suppliers\": [\n",
    "        \"SupplierID\", \"CompanyName\", \"ContactName\", \"ContactTitle\", \"Address\", \"City\", \"Region\", \"PostalCode\",\n",
    "        \"Country\", \"Phone\", \"Fax\", \"HomePage\"\n",
    "    ],\n",
    "    \"Order Details\": [\"OrderID\", \"ProductID\", \"UnitPrice\", \"Quantity\", \"Discount\"],\n",
    "    \"Region\": [\"RegionID\", \"RegionDescription\"],\n",
    "    \"Territories\": [\"TerritoryID\", \"TerritoryDescription\", \"RegionID\"],\n",
    "    \"EmployeeTerritories\": [\"EmployeeID\", \"TerritoryID\"]\n",
    "}\n",
    "\n",
    "# Preprocess the natural language query\n",
    "def preprocess_query(natural_language_query):\n",
    "    doc = nlp_spacy(natural_language_query)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":  # Geopolitical entity (city, country)\n",
    "            natural_language_query = natural_language_query.replace(ent.text, f\"'{ent.text}'\")\n",
    "    for table, columns in TABLE_COLUMN_MAPPING.items():\n",
    "        natural_language_query = re.sub(rf\"\\b{table}\\b\", table, natural_language_query, flags=re.IGNORECASE)\n",
    "        for column in columns:\n",
    "            natural_language_query = re.sub(rf\"\\b{column}\\b\", column, natural_language_query, flags=re.IGNORECASE)\n",
    "    stopwords = [\"list of\", \"show me\", \"all the\", \"who\"]\n",
    "    for word in stopwords:\n",
    "        natural_language_query = natural_language_query.replace(word, \"\")\n",
    "    return natural_language_query.strip()\n",
    "\n",
    "# Convert natural language to SQL\n",
    "def nl_to_sql(natural_language_query):\n",
    "    try:\n",
    "        cleaned_query = preprocess_query(natural_language_query)\n",
    "        print(f\"line 90 Cleaned Query: {cleaned_query}\")\n",
    "        \n",
    "        # Prompt for the model\n",
    "        prompt = (\n",
    "            f\"Translate the following natural language question into an SQL query for Microsoft SQL Server. \"\n",
    "            f\"Use correct syntax without backticks: {cleaned_query}.\\n\"\n",
    "            f\"Ensure the query is well-formatted for this database schema: {TABLE_COLUMN_MAPPING}\"\n",
    "        )\n",
    "        \n",
    "        # Generate content using the model\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Clean up the response\n",
    "        sql_query = response.text.strip()\n",
    "        sql_query = sql_query.replace(\"`\", \"\")  # Remove any backticks\n",
    "        # Ensure no unwanted prefixes like \"sql\" in the output\n",
    "        sql_query = sql_query.lstrip(\"sql\").strip()\n",
    "        print(f\"line 107 sql querry generated {sql_query}\")\n",
    "        return sql_query\n",
    "    except Exception as e:\n",
    "        print(f\"line 110 Error generating SQL query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Execute the SQL query\n",
    "def execute_query(query):\n",
    "    # query='SELECT * FROM Employees'\n",
    "    try:\n",
    "        try:\n",
    "            cursor = conn_cursor().cursor()\n",
    "            cursor.execute(query)\n",
    "            df=cursor.fetchall()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'line 123 exception occours as {e}')\n",
    "            engine = create_connection()\n",
    "            if engine is None:\n",
    "                df = pd.DataFrame()\n",
    "            else:\n",
    "                df = pd.read_sql(query, engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"line 131 Error executing SQL query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Handle user's natural language query\n",
    "def handle_query(natural_language_query):\n",
    "    print(f\"User Query: {natural_language_query}\")\n",
    "    sql_query = nl_to_sql(natural_language_query)\n",
    "    if not sql_query:\n",
    "        return \"Failed to generate SQL query. Please try again.\"\n",
    "    print(f\"line 140 Generated SQL Query: {sql_query}\")\n",
    "    result_df = execute_query(sql_query)\n",
    "    if not result_df:\n",
    "        return \"No results found or query execution failed.\"\n",
    "    return result_df\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     user_query = \"Show me the list of all the \"\n",
    "#     result = handle_query(user_query)\n",
    "#     print(f\"Query Result:\\n{result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfOewvNFMpz1"
   },
   "source": [
    "#it is the demo function by gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "03lLbtNMhtUw"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 1. SETTING UP DATABASE\n",
    "# -----------------------------------\n",
    "def setup_database():\n",
    "    \"\"\"\n",
    "    Creates a SQLite database to store complaints. The table `complaints` will\n",
    "    store complaint ID, customer email, subject, body, and timestamp.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect('customer_care.db')  # Creates or connects to the database\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS complaints (\n",
    "            id TEXT PRIMARY KEY,  -- Unique ID for the complaint\n",
    "            email TEXT,           -- Customer email address\n",
    "            subject TEXT,         -- Subject of the email\n",
    "            body TEXT,            -- Body/content of the email\n",
    "            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP -- Timestamp of the complaint\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TNJMBBD5hyei"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 2. FETCHING UNREAD EMAILS\n",
    "# -----------------------------------\n",
    "def fetch_emails():\n",
    "    \"\"\"\n",
    "    Connects to the Gmail IMAP server, fetches unread emails from the inbox,\n",
    "    and extracts their content (subject, sender, and body).\n",
    "    \"\"\"\n",
    "    imap = imaplib.IMAP4_SSL('imap.gmail.com')\n",
    "    imap.login('saqlainfawad@gmail.com', 'jkgc keje xjmz xqyp')  # Replace with your credentials\n",
    "    imap.select('inbox')\n",
    "    _, messages = imap.search(None, 'UNSEEN')  # Fetch unread emails\n",
    "    # print(f' line 14 messages :: {messages}')\n",
    "    # print(f' line 15 messages :: {_}')\n",
    "\n",
    "    email_ids = messages[0].split()\n",
    "    # print(f' line 18 email_ids :: {email_ids}')\n",
    "    emails = []\n",
    "    for eid in email_ids:\n",
    "        _, msg_data = imap.fetch(eid, '(RFC822)')\n",
    "        # print(f' line 22 msg_data :: {msg_data}')\n",
    "        # print(f' line 23 msg_data :: {_}')\n",
    "        raw_email = msg_data[0][1]\n",
    "        # print(f' line 25 raw_email :: {raw_email}')\n",
    "        msg = email.message_from_bytes(raw_email)\n",
    "        # print(f' line 27 msg :: {msg}')\n",
    "\n",
    "        # Extract the body of the email\n",
    "        body = None\n",
    "        if msg.is_multipart():  # For emails with multiple parts\n",
    "            for part in msg.walk():\n",
    "                content_type = part.get_content_type()\n",
    "                # print(f' line 34 content_type :: {content_type}')\n",
    "                content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "                # print(f' line 36 content_disposition :: {content_disposition}')\n",
    "\n",
    "                if content_type == \"text/plain\" and \"attachment\" not in content_disposition:\n",
    "                    raw_body = part.get_payload(decode=True)\n",
    "                    # print(f' line 40 raw_body :: {raw_body}')\n",
    "                    if raw_body:\n",
    "                        encoding = chardet.detect(raw_body)['encoding'] or 'utf-8'\n",
    "                        body = raw_body.decode(encoding, errors='replace')\n",
    "                        # print(f' line 44 body :: {body}')\n",
    "                        break\n",
    "        else:  # For single-part emails\n",
    "            raw_body = msg.get_payload(decode=True)\n",
    "            if raw_body:\n",
    "                encoding = chardet.detect(raw_body)['encoding'] or 'utf-8'\n",
    "                # print(f' line 50 encoding :: {encoding}')\n",
    "                body = raw_body.decode(encoding, errors='replace')\n",
    "                # print(f' line 52 body :: {body}')\n",
    "\n",
    "        emails.append({\n",
    "            'subject': msg['subject'] or \"No Subject\",\n",
    "            'from': msg['from'] or \"Unknown Sender\",\n",
    "            'body': body or \"No Content\"\n",
    "        })\n",
    "        # print(f' line 59 emails :: {emails}')\n",
    "    imap.close()\n",
    "    imap.logout()\n",
    "    return emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vp3eE2y6h295"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 3. SENDING EMAILS\n",
    "# -----------------------------------\n",
    "def send_email(to_email, subject, body):\n",
    "    \"\"\"\n",
    "    Sends an email using SMTP. The email includes the recipient's address,\n",
    "    subject, and body content.\n",
    "    \"\"\"\n",
    "    msg = EmailMessage()\n",
    "    msg['From'] = 'your_email@example.com'  # Replace with your email\n",
    "    msg['To'] = to_email\n",
    "    msg['Subject'] = subject\n",
    "    msg.set_content(body)\n",
    "\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n",
    "        smtp.login('saqlainfawad@gmail.com', 'jkgc keje xjmz xqyp')  # Replace with your credentials\n",
    "        smtp.send_message(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "neahN9Msh8U1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 4. CLASSIFYING EMAILS\n",
    "# -----------------------------------\n",
    "from transformers import pipeline\n",
    "\n",
    "def classify_email(email_body):\n",
    "    \"\"\"\n",
    "    Classifies emails in chunks if the email body is too long.\n",
    "    \"\"\"\n",
    "    classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    \n",
    "    # Split email body into chunks (512 tokens max per chunk)\n",
    "    max_chunk_size = 512\n",
    "    chunks = [email_body[i:i + max_chunk_size] for i in range(0, len(email_body), max_chunk_size)]\n",
    "    \n",
    "    results = [classifier(chunk)[0]['label'] for chunk in chunks]\n",
    "    \n",
    "    # Aggregate results: If most chunks are \"NEGATIVE\", classify as Complaint\n",
    "    if results.count('NEGATIVE') > len(results) / 2:\n",
    "        return 'Complaint'\n",
    "    else:\n",
    "        return 'Query'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0BsUezz-h-my"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 5. LOGGING COMPLAINTS\n",
    "# -----------------------------------\n",
    "def log_complaint(email, subject, body):\n",
    "    \"\"\"\n",
    "    Logs complaints into the database with a unique ID.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect('customer_care.db')\n",
    "    cursor = conn.cursor()\n",
    "    complaint_id = str(uuid.uuid4())  # Generate a unique ID for the complaint\n",
    "    cursor.execute('''\n",
    "        INSERT INTO complaints (id, email, subject, body)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    ''', (complaint_id, email, subject, body))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return complaint_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FQpDpNk7iCVH"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# 6. GENERATING QUERY RESPONSES\n",
    "# -----------------------------------\n",
    "def generate_response(query):\n",
    "    \"\"\"\n",
    "    Generates a response for queries using the DialoGPT model. This provides a\n",
    "    conversational response based on the query.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "    # Tokenize and truncate input sequence to the maximum allowed length (700 tokens)\n",
    "    input_ids = tokenizer.encode(query, return_tensors='pt', max_length=512, truncation=True)\n",
    "    response_ids = model.generate(input_ids, max_length=100)\n",
    "    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    print(f' line 17 response :: {response}')\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "VsNhqXGdUBQW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 7. MAIN PROCESSING FUNCTION\n",
    "# -----------------------------------\n",
    "def process_emails():\n",
    "    \"\"\"\n",
    "    Main function to process emails:\n",
    "    1. Fetch unread emails.\n",
    "    2. Classify each email as a complaint or query.\n",
    "    3. Log complaints into the database and send acknowledgment.\n",
    "    4. Generate responses for queries and send them back to the sender.\n",
    "    \"\"\"\n",
    "    setup_database()  # Ensure the database is ready\n",
    "    emails = fetch_emails()  # Fetch unread emails\n",
    "\n",
    "    for email_data in emails:\n",
    "        subject = email_data['subject']\n",
    "        sender = email_data['from']\n",
    "        body = email_data['body']\n",
    "\n",
    "        # Classify email\n",
    "        classification = classify_email(body)\n",
    "\n",
    "        if classification == 'Complaint':\n",
    "            # Log complaint and send acknowledgment\n",
    "            complaint_id = log_complaint(sender, subject, body)\n",
    "            response = f\"Dear Customer,\\n\\nYour complaint has been logged with the ID: {complaint_id}.\\nWe will address it as soon as possible.\\n\\nThank you.\"\n",
    "        elif classification == 'Query':\n",
    "            # Example usage\n",
    "            if __name__ == \"__main__\":\n",
    "                user_query = body\n",
    "                result = handle_query(user_query)\n",
    "                print(f\"Query Result:\\n{result}\")\n",
    "                response = f\"Dear Customer,\\n\\n{result}.\\n\\nThank you.\"\n",
    "                \n",
    "        else:\n",
    "            # Generate query response\n",
    "            response = generate_response(body)\n",
    "\n",
    "        # Send response email\n",
    "        send_email(sender, f\"Re: {subject}\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "icEmzHwKiFK2",
    "outputId": "f748be7a-a2fe-4a35-a63a-15a2ad4aea0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 8. RUN THE SYSTEM\n",
    "# -----------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        process_emails()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNN+lRYnQbyhoOQXtAdAE0J",
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
